"""
–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è AI-—É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ, –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –¥—Ä—É–≥–∏–µ –ø–æ–ª—è –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π —Å—Ä–µ–¥—Å—Ç–≤ –ò–ò
"""

import re
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class AIDetectionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ AI-—É—è–∑–≤–∏–º–æ—Å—Ç–∏"""
    is_ai_related: bool
    confidence: float  # 0.0 - 1.0
    matched_keywords: List[str]
    matched_categories: List[str]
    suggested_tags: List[str]
    risk_multiplier: float  # –ú–Ω–æ–∂–∏—Ç–µ–ª—å —Ä–∏—Å–∫–∞ –¥–ª—è AI-—É—è–∑–≤–∏–º–æ—Å—Ç–µ–π


class AITaggerService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Ç–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è AI-—É—è–∑–≤–∏–º–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ AI –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –≤–µ—Å–∞–º–∏ (—á–µ–º –≤—ã—à–µ –≤–µ—Å, —Ç–µ–º –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
        self.ai_keywords = self._init_ai_keywords()
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π AI
        self.ai_attack_patterns = self._init_attack_patterns()
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        self.confidence_threshold = 0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è is_ai_related
        self.high_confidence_threshold = 0.7  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    
    def _init_ai_keywords(self) -> Dict[str, Dict[str, float]]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        return {
            # === –ö–†–ò–¢–ò–ß–ù–´–ï AI –¢–ï–†–ú–ò–ù–´ (–≤–µ—Å 1.0) ===
            'core_ai': {
                'artificial intelligence': 1.0,
                'machine learning': 1.0,
                'deep learning': 1.0,
                'neural network': 1.0,
                'llm': 1.0,
                'large language model': 1.0,
                'foundation model': 1.0,
                'generative ai': 1.0,
                'ai model': 1.0,
                'transformer': 0.9,
                'reinforcement learning': 0.9,
            },
            
            # === AI –ü–õ–ê–¢–§–û–†–ú–´ –ò –§–†–ï–ô–ú–í–û–†–ö–ò (–≤–µ—Å 0.9) ===
            'platforms': {
                'tensorflow': 0.9,
                'pytorch': 0.9,
                'keras': 0.8,
                'scikit-learn': 0.8,
                'huggingface': 0.9,
                'hugging face': 0.9,
                'openai': 0.9,
                'anthropic': 0.8,
                'langchain': 0.9,
                'llamaindex': 0.8,
                'autogpt': 0.8,
            },
            
            # === AI –ü–†–û–î–£–ö–¢–´ (–≤–µ—Å 0.85) ===
            'products': {
                'gpt': 0.9,
                'chatgpt': 0.9,
                'gpt-3': 0.9,
                'gpt-4': 0.9,
                'claude': 0.8,
                'bard': 0.8,
                'gemini': 0.8,
                'dall-e': 0.8,
                'stable diffusion': 0.8,
                'midjourney': 0.7,
                'whisper': 0.7,
            },
            
            # === AI –ê–¢–ê–ö–ò (–≤–µ—Å 1.0 - –æ—á–µ–Ω—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ) ===
            'attacks': {
                'prompt injection': 1.0,
                'jailbreak': 1.0,
                'adversarial': 0.95,
                'adversarial attack': 0.95,
                'adversarial example': 0.95,
                'model extraction': 0.95,
                'model inversion': 0.95,
                'membership inference': 0.95,
                'data poisoning': 0.95,
                'backdoor attack': 0.95,
                'model poisoning': 0.95,
                'training data leak': 0.9,
                'model stealing': 0.9,
                'guardrail bypass': 0.9,
            },
            
            # === AI –ö–û–ú–ü–û–ù–ï–ù–¢–´ (–≤–µ—Å 0.6 - –º–æ–∂–µ—Ç –±—ã—Ç—å false positive) ===
            'components': {
                'embedding': 0.6,
                'attention mechanism': 0.7,
                'convolutional': 0.6,
                'recurrent': 0.6,
                'lstm': 0.7,
                'gru': 0.7,
                'gan': 0.7,
                'autoencoder': 0.7,
                'gradient descent': 0.5,
                'backpropagation': 0.5,
            },
            
            # === AI –ò–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–ê (–≤–µ—Å 0.5 - –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã) ===
            'infrastructure': {
                'model serving': 0.7,
                'model inference': 0.7,
                'model training': 0.6,
                'fine-tuning': 0.6,
                'vector database': 0.7,
                'chromadb': 0.7,
                'pinecone': 0.7,
                'weaviate': 0.7,
                'faiss': 0.7,
            },
            
            # === AI –û–ë–õ–ê–°–¢–ò –ü–†–ò–ú–ï–ù–ï–ù–ò–Ø (–≤–µ—Å 0.5) ===
            'applications': {
                'computer vision': 0.6,
                'natural language processing': 0.6,
                'nlp': 0.6,
                'speech recognition': 0.6,
                'autonomous': 0.5,
                'self-driving': 0.6,
                'recommendation system': 0.5,
            }
        }
    
    def _init_attack_patterns(self) -> Dict[str, List[str]]:
        """–ü–∞—Ç—Ç–µ—Ä–Ω—ã –∞—Ç–∞–∫ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è AI"""
        return {
            'prompt_attacks': [
                'prompt injection', 'jailbreak', 'prompt leak',
                'system prompt', 'role playing', 'dan mode'
            ],
            'model_attacks': [
                'model extraction', 'model inversion', 'model stealing',
                'adversarial', 'membership inference', 'model poisoning'
            ],
            'data_attacks': [
                'data poisoning', 'training data leak', 'backdoor',
                'trojan model', 'poisoned model'
            ],
            'inference_attacks': [
                'inference manipulation', 'output parser exploit',
                'token smuggling', 'context overflow'
            ]
        }
    
    def analyze_vulnerability(self, 
                            title: str = "", 
                            description: str = "", 
                            cve_id: str = "",
                            affected_software: Optional[List[str]] = None,
                            references: Optional[List[str]] = None) -> AIDetectionResult:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç—å –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∫ AI
        
        Args:
            title: –ù–∞–∑–≤–∞–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
            description: –û–ø–∏—Å–∞–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏
            cve_id: CVE –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            affected_software: –°–ø–∏—Å–æ–∫ –∑–∞—Ç—Ä–æ–Ω—É—Ç–æ–≥–æ –ü–û
            references: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫
            
        Returns:
            AIDetectionResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        affected_software = affected_software or []
        references = references or []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        combined_text = f"{title} {description} {cve_id} {' '.join(affected_software)} {' '.join(references)}"
        combined_text = combined_text.lower()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matched_keywords = []
        confidence_scores = []
        matched_categories = []
        
        for category, keywords in self.ai_keywords.items():
            for keyword, weight in keywords.items():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                pattern = r'\b' + re.escape(keyword) + r'\b'
                if re.search(pattern, combined_text, re.IGNORECASE):
                    matched_keywords.append(keyword)
                    confidence_scores.append(weight)
                    if category not in matched_categories:
                        matched_categories.append(category)
                    
                    self.logger.debug(f"Found keyword: {keyword} (category: {category}, weight: {weight})")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if confidence_scores:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å + –±–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            max_confidence = max(confidence_scores)
            quantity_bonus = min(len(confidence_scores) * 0.05, 0.3)  # –î–æ +0.3 –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            total_confidence = min(max_confidence + quantity_bonus, 1.0)
        else:
            total_confidence = 0.0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ AI-—É—è–∑–≤–∏–º–æ—Å—Ç—å—é
        is_ai_related = total_confidence >= self.confidence_threshold
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–≥–∏
        suggested_tags = self._generate_tags(
            matched_keywords, 
            matched_categories, 
            combined_text
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å —Ä–∏—Å–∫–∞
        risk_multiplier = self._calculate_risk_multiplier(
            matched_categories,
            total_confidence
        )
        
        result = AIDetectionResult(
            is_ai_related=is_ai_related,
            confidence=total_confidence,
            matched_keywords=matched_keywords[:10],  # –¢–æ–ø-10 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            matched_categories=matched_categories,
            suggested_tags=suggested_tags,
            risk_multiplier=risk_multiplier
        )
        
        if is_ai_related:
            self.logger.info(
                f"ü§ñ AI-—É—è–∑–≤–∏–º–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞: "
                f"Confidence={total_confidence:.2f}, "
                f"Keywords={len(matched_keywords)}, "
                f"Tags={suggested_tags}"
            )
        
        return result
    
    def _generate_tags(self, 
                       matched_keywords: List[str], 
                       categories: List[str],
                       text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        tags = set()
        
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–≥ AI
        if matched_keywords:
            tags.add('ai')
        
        # –¢–µ–≥–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if 'core_ai' in categories or 'platforms' in categories:
            tags.add('machine_learning')
            tags.add('neural_network')
        
        if 'products' in categories:
            tags.add('ai_product')
        
        if 'attacks' in categories:
            tags.add('ai_attack')
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ–≥–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –∞—Ç–∞–∫
        for attack_type, patterns in self.ai_attack_patterns.items():
            if any(pattern in text for pattern in patterns):
                tags.add(attack_type)
        
        # –¢–µ–≥–∏ –ø–æ —Ç–∏–ø–∞–º –ø–ª–∞—Ç—Ñ–æ—Ä–º
        if any(kw in matched_keywords for kw in ['tensorflow', 'pytorch', 'keras']):
            tags.add('ml_framework')
        
        if any(kw in matched_keywords for kw in ['gpt', 'chatgpt', 'llm', 'large language model']):
            tags.add('llm')
        
        if any(kw in matched_keywords for kw in ['langchain', 'llamaindex', 'autogpt']):
            tags.add('ai_agent')
        
        # –¢–µ–≥–∏ –ø–æ —Ç–∏–ø–∞–º –∞—Ç–∞–∫
        if 'prompt injection' in text or 'jailbreak' in text:
            tags.add('prompt_injection')
            tags.add('critical_ai_attack')
        
        if 'adversarial' in text:
            tags.add('adversarial_attack')
        
        if any(kw in text for kw in ['model extraction', 'model stealing', 'model inversion']):
            tags.add('model_theft')
        
        return sorted(list(tags))
    
    def _calculate_risk_multiplier(self, 
                                   categories: List[str],
                                   confidence: float) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–Ω–æ–∂–∏—Ç–µ–ª—å —Ä–∏—Å–∫–∞ –¥–ª—è AI-—É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
        AI-—É—è–∑–≤–∏–º–æ—Å—Ç–∏ –æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã
        """
        base_multiplier = 1.0
        
        # –ü–æ–≤—ã—à–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∞—Ç–∞–∫
        if 'attacks' in categories:
            base_multiplier = 2.0
        elif 'core_ai' in categories or 'platforms' in categories:
            base_multiplier = 1.5
        elif 'products' in categories:
            base_multiplier = 1.3
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_bonus = confidence * 0.5  # –î–æ +0.5x
        
        return base_multiplier + confidence_bonus
    
    def batch_analyze(self, vulnerabilities: List[Dict]) -> List[Tuple[Dict, AIDetectionResult]]:
        """
        –ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ø–∏—Å–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
        
        Args:
            vulnerabilities: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —É—è–∑–≤–∏–º–æ—Å—Ç—è–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—É—è–∑–≤–∏–º–æ—Å—Ç—å, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞)
        """
        results = []
        
        for vuln in vulnerabilities:
            result = self.analyze_vulnerability(
                title=vuln.get('title', ''),
                description=vuln.get('description', ''),
                cve_id=vuln.get('cve_id', ''),
                affected_software=vuln.get('affected_software', []),
                references=vuln.get('references', [])
            )
            results.append((vuln, result))
        
        ai_count = sum(1 for _, r in results if r.is_ai_related)
        self.logger.info(f"Batch analysis: {ai_count}/{len(vulnerabilities)} AI-—É—è–∑–≤–∏–º–æ—Å—Ç–µ–π")
        
        return results
    
    def get_statistics(self, results: List[AIDetectionResult]) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞"""
        total = len(results)
        ai_related = sum(1 for r in results if r.is_ai_related)
        
        if not results:
            return {
                'total': 0,
                'ai_related': 0,
                'percentage': 0.0,
                'avg_confidence': 0.0,
                'categories': {},
                'tags': {}
            }
        
        avg_confidence = sum(r.confidence for r in results if r.is_ai_related) / max(ai_related, 1)
        
        # –ü–æ–¥—Å—á–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_counts = {}
        tag_counts = {}
        
        for result in results:
            if result.is_ai_related:
                for cat in result.matched_categories:
                    category_counts[cat] = category_counts.get(cat, 0) + 1
                for tag in result.suggested_tags:
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        return {
            'total': total,
            'ai_related': ai_related,
            'percentage': (ai_related / total * 100) if total > 0 else 0,
            'avg_confidence': avg_confidence,
            'categories': category_counts,
            'tags': tag_counts
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
ai_tagger = AITaggerService()
